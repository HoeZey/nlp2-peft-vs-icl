============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
INFO 06-01 14:12:25 [__init__.py:239] Automatically detected platform cuda.
Namespace(model_path='meta-llama/Llama-3.1-8B', system_prompt_path='config/prompts/math.txt', lora_path='lora_checkpoints/run-1/', num_gpus=2, eval_dataset='nlile/hendrycks-MATH-benchmark', eval_dataset_config_name=None, eval_split='test', icl_dataset=None, icl_dataset_config_name=None, icl_split='train', k=0, sampling_strategy='random', embedder='math-similarity/Bert-MLM_arXiv-MP-class_zbMath', batch_size=500, n=5, max_tokens=8000, temperature=0.6, top_p=0.95, seed=42, output_dir='./model_outputs/math')
Using MATH processor
INFO 06-01 14:12:40 [config.py:689] This model supports multiple tasks: {'reward', 'embed', 'classify', 'score', 'generate'}. Defaulting to 'generate'.
WARNING 06-01 14:12:40 [arg_utils.py:1603] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.
INFO 06-01 14:12:40 [config.py:1713] Defaulting to use mp for distributed inference
INFO 06-01 14:12:40 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='meta-llama/Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:12:41 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks
INFO 06-01 14:12:42 [cuda.py:292] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:12:42 [cuda.py:292] Using Flash Attention backend.
INFO 06-01 14:12:44 [utils.py:993] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:12:44 [utils.py:993] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:12:44 [pynccl.py:69] vLLM is using nccl==2.21.5
INFO 06-01 14:12:44 [pynccl.py:69] vLLM is using nccl==2.21.5
INFO 06-01 14:12:45 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/scur2187/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:12:45 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/scur2187/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 06-01 14:12:45 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_2aefa33b'), local_subscribe_addr='ipc:///scratch-local/scur2187.12116203/ade52da1-8f13-4d9a-9798-a61db8c9cc73', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 06-01 14:12:45 [parallel_state.py:959] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:12:45 [parallel_state.py:959] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1
INFO 06-01 14:12:45 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B...
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:12:45 [model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B...
INFO 06-01 14:12:45 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:12:45 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.71s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.74s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:06<00:01,  1.77s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.19s/it]

INFO 06-01 14:12:54 [loader.py:458] Loading weights took 8.79 seconds
INFO 06-01 14:12:54 [punica_selector.py:18] Using PunicaWrapperGPU.
INFO 06-01 14:12:54 [model_runner.py:1146] Model loading took 7.5857 GiB and 9.497936 seconds
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:12:54 [loader.py:458] Loading weights took 8.94 seconds
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:12:54 [punica_selector.py:18] Using PunicaWrapperGPU.
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:12:55 [model_runner.py:1146] Model loading took 7.5857 GiB and 9.946021 seconds
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:13:08 [worker.py:267] Memory profiling takes 13.41 seconds
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:13:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.11GiB) x gpu_memory_utilization (0.90) = 83.80GiB
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:13:08 [worker.py:267] model weights take 7.59GiB; non_torch_memory takes 0.78GiB; PyTorch activation peak memory takes 8.28GiB; the rest of the memory reserved for KV Cache is 67.15GiB.
INFO 06-01 14:13:08 [worker.py:267] Memory profiling takes 13.42 seconds
INFO 06-01 14:13:08 [worker.py:267] the current vLLM instance can use total_gpu_memory (93.11GiB) x gpu_memory_utilization (0.90) = 83.80GiB
INFO 06-01 14:13:08 [worker.py:267] model weights take 7.59GiB; non_torch_memory takes 0.84GiB; PyTorch activation peak memory takes 8.28GiB; the rest of the memory reserved for KV Cache is 67.09GiB.
INFO 06-01 14:13:09 [executor_base.py:112] # cuda blocks: 68698, # CPU blocks: 4096
INFO 06-01 14:13:09 [executor_base.py:117] Maximum concurrency for 131072 tokens per request: 8.39x
INFO 06-01 14:13:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:13:10 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:25,  1.34it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:04<01:13,  2.24s/it]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:04<00:47,  1.49s/it]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:05<00:35,  1.14s/it]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:05<00:28,  1.05it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:06<00:24,  1.20it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:07<00:21,  1.32it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:07<00:19,  1.41it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:08<00:17,  1.48it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:08<00:16,  1.52it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:09<00:15,  1.56it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:10<00:14,  1.59it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:10<00:13,  1.61it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:11<00:12,  1.62it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:11<00:12,  1.63it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:12<00:11,  1.64it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:13<00:10,  1.64it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:14<00:16,  1.03it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:16<00:19,  1.24s/it]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:17<00:15,  1.05s/it]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:18<00:12,  1.09it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:18<00:10,  1.21it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:19<00:09,  1.32it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:19<00:07,  1.40it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:20<00:06,  1.47it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:21<00:05,  1.52it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:21<00:05,  1.54it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:22<00:04,  1.57it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:22<00:03,  1.60it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:23<00:03,  1.60it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:24<00:02,  1.61it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:24<00:01,  1.62it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:25<00:01,  1.64it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:25<00:00,  1.65it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:31<00:00,  1.98s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:31<00:00,  1.13it/s]
INFO 06-01 14:13:44 [custom_all_reduce.py:195] Registering 2275 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:13:44 [custom_all_reduce.py:195] Registering 2275 cuda graph addresses
INFO 06-01 14:13:44 [model_runner.py:1598] Graph capturing finished in 34 secs, took 0.41 GiB
[1;36m(VllmWorkerProcess pid=1900969)[0;0m INFO 06-01 14:13:44 [model_runner.py:1598] Graph capturing finished in 34 secs, took 0.41 GiB
INFO 06-01 14:13:44 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 49.33 seconds
Processing batch 0
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 12116203 ON gcn118 CANCELLED AT 2025-06-01T14:15:23 ***
Processed prompts:   0%|          | 0/2500 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]slurmstepd: error: *** STEP 12116203.0 ON gcn118 CANCELLED AT 2025-06-01T14:15:23 ***

JOB STATISTICS
==============
Job ID: 12116203
Cluster: snellius
User/Group: scur2187/scur2187
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 16
CPU Utilized: 00:04:32
CPU Efficiency: 8.59% of 00:52:48 core-walltime
Job Wall-clock time: 00:03:18
Memory Utilized: 11.29 GB
Memory Efficiency: 3.14% of 360.00 GB (360.00 GB/node)
